A ieee journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract journal abstract Big Data Guide to Understanding Climate Change The Case for Theory-Guided Data Science Abstract Global climate change and its impact on human life has become one of our era's greatest challenges Despite the urgency data science has had little impact on furthering our understanding of our planet in spite of the abundance of climate data This is a stark contrast from other fields such as advertising or electronic commerce where big data has been a great success story This discrepancy stems from the complex nature of climate data as well as the scientific questions climate science brings forth This article introduces a data science audience to the challenges and opportunities to mine large climate datasets with an emphasis on the nuanced difference between mining climate data and traditional big data approaches We focus on data methods and application challenges that must be addressed in order for big data to fulfill their promise with regard to climate science applications More importantly we highlight research showing that solely relying on traditional big data techniques results in dubious findings and we instead propose a theory-guided data science paradigm that uses scientific theory to constrain both the big data techniques as well as the results-interpretation process to extract accurate insight from large climate data,
Forecasting Significant Societal Events Using The Embers Streaming Predictive Analytics System Abstract Developed under the Intelligence Advanced Research Project Activity Open Source Indicators program Early Model Based Event Recognition using Surrogates EMBERS is a large-scale big data analytics system for forecasting significant societal events such as civil unrest events on the basis of continuous automated analysis of large volumes of publicly available data It has been operational since November  and delivers approximately  predictions each day for countries of Latin America EMBERS is built on a streaming scalable loosely coupled shared-nothing architecture using ZeroMQ as its messaging backbone and JSON as its wire data format It is deployed on Amazon Web Services using an entirely automated deployment process We describe the architecture of the system some of the design tradeoffs encountered during development and specifics of the machine learning models underlying EMBERS We also present a detailed prospective evaluation of EMBERS in forecasting significant societal events in the past  years,
Data Integration for Heterogenous Datasets Abstract More and more the needs of data analysts are requiring the use of data outside the control of their own organizations The increasing amount of data available on the Web the new technologies for linking data across datasets and the increasing need to integrate structured and unstructured data are all driving this trend In this article we provide a technical overview of the emerging broad data area in which the variety of heterogeneous data being used rather than the scale of the data being analyzed is the limiting factor in data analysis efforts The article explores some of the emerging themes in data discovery data integration linked data and the combination of structured and unstructured data,
Toward a Literature-Driven Definition of Big Data in Healthcare Objective The aim of this study was to provide a definition of big data in healthcare Methods A systematic search of PubMed literature published until May   was conducted We noted the number of statistical individuals n and the number of variables p for all papers describing a dataset These papers were classified into fields of study Characteristics attributed to big data by authors were also considered Based on this analysis a definition of big data was proposed Results A total of  papers were included Big data can be defined as datasets with Lognp   Properties of big data are its great variety and high velocity Big data raises challenges on veracity on all aspects of the workflow on extracting meaningful information and on sharing information Big data requires new computational methods that optimize data management Related concepts are data reuse false knowledge discovery and privacy issues Conclusion Big data is defined by volume Big data should not be confused with data reuse data can be big without being reused for another purpose for example in omics Inversely data can be reused without being necessarily big for example secondary use of Electronic Medical Records EMR data,
Big Data Survey Technologies Opportunities and Challenges Big Data has gained much attention from the academia and the IT industry In the digital and computing world information is generated and collected at a rate that rapidly exceeds the boundary range Currently over  billion people worldwide are connected to the Internet and over  billion individuals own mobile phones By   billion devices are expected to be connected to the Internet At this point predicted data production will be  times greater than that in  As information is transferred and shared at light speed on optic fiber and wireless networks the volume of data and the speed of market growth increase However the fast growth rate of such large data generates numerous challenges such as the rapid growth of data transfer speed diverse data and security Nonetheless Big Data is still in its infancy stage and the domain has not been reviewed in general Hence this study comprehensively surveys and classifies the various attributes of Big Data including its nature definitions rapid growth rate volume management analysis and security This study also proposes a data life cycle that uses the technologies and terminologies of Big Data Future research directions in this field are determined based on opportunities and several open issues in Big Data domination These research directions facilitate the exploration of the domain and the development of optimal techniques to address Big Data,
Lessons Learned About Public Health from Online Crowd Surveillance Abstract The Internet has forever changed the way people access information and make decisions about their healthcare needs Patients now share information about their health at unprecedented rates on social networking sites such as Twitter and Facebook and on medical discussion boards In addition to explicitly shared information about health conditions through posts patients reveal data on their inner fears and desires about health when searching for health-related keywords on search engines Data are also generated by the use of mobile phone applications that track users' health behaviors eg eating and exercise habits as well as give medical advice The data generated through these applications are mined and repackaged by surveillance systems developed by academics companies and governments alike to provide insight to patients and healthcare providers for medical decisions Until recently most Internet research in public health has been surveillance focused or monitoring health behaviors Only recently have researchers used and interacted with the crowd to ask questions and collect health-related data In the future we expect to move from this surveillance focus to the ideal of Internet-based patient-level interventions where healthcare providers help patients change their health behaviors In this article we highlight the results of our prior research on crowd surveillance and make suggestions for the future,
Big Data Analysis Framework for Healthcare and Social Sectors in Korea Objectives We reviewed applications of big data analysis of healthcare and social services in developed countries and subsequently devised a framework for such an analysis in Korea Methods We reviewed the status of implementing big data analysis of health care and social services in developed countries and strategies used by the Ministry of Health and Welfare of Korea Government  We formulated a conceptual framework of big data in the healthcare and social service sectors at the national level As a specific case we designed a process and method of social big data analysis on suicide buzz Results Developed countries eg the United States the UK Singapore Australia and even OECD and EU are emphasizing the potential of big data and using it as a tool to solve their long-standing problems Big data strategies for the healthcare and social service sectors were formulated based on an ICT-based policy of current government and the strategic goals of the Ministry of Health and Welfare We suggest a framework of big data analysis in the healthcare and welfare service sectors separately and assigned them tentative names 'health risk analysis center' and 'integrated social welfare service network' A framework of social big data analysis is presented by applying it to the prevention and proactive detection of suicide in Korea Conclusions There are some concerns with the utilization of big data in the healthcare and social welfare sectors Thus research on these issues must be conducted so that sophisticated and practical solutions can be reached,
Hard Data Analytics Problems Make for Better Data Analysis Algorithms Bioinformatics as an Example Abstract Data mining and knowledge discovery techniques have greatly progressed in the last decade They are now able to handle larger and larger datasets process heterogeneous information integrate complex metadata and extract and visualize new knowledge Often these advances were driven by new challenges arising from real-world domains with biology and biotechnology a prime source of diverse and hard eg high volume high throughput high variety and high noise data analytics problems The aim of this article is to show the broad spectrum of data mining tasks and challenges present in biological data and how these challenges have driven us over the years to design new data mining and knowledge discovery procedures for biodata This is illustrated with the help of two kinds of case studies The first kind is focused on the field of protein structure prediction where we have contributed in several areas by designing through regression functions that can distinguish between good and bad models of a protein's predicted structure by creating new measures to characterize aspects of a protein's structure associated with individual positions in a protein's sequence measures containing information that might be useful for protein structure prediction and by creating accurate estimators of these structural aspects The second kind of case study is focused on omics data analytics a class of biological data characterized for having extremely high dimensionalities Our methods were able not only to generate very accurate classification models but also to discover new biological knowledge that was later ratified by experimentalists Finally we describe several strategies to tightly integrate knowledge extraction and data mining in order to create a new class of biodata mining algorithms that can natively embrace the complexity of biological data efficiently generate accurate information in the form of classificationregression models and extract valuable new knowledge Thus a complete data-to-information-to-knowledge pipeline is presented,
Structured Open Urban Data Understanding the Landscape Abstract A growing number of cities are now making urban data freely available to the public Besides promoting transparency these data can have a transformative effect in social science research as well as in how citizens participate in governance These initiatives however are fairly recent and the landscape of open urban data is not well known In this study we try to shed some light on this through a detailed study of over  open data sets from  cities in North America We start by presenting general statistics about the content size nature and popularity of the different data sets and then examine in more detail structured data sets that contain tabular data Since a key benefit of having a large number of data sets available is the ability to fuse information we investigate opportunities for data integration We also study data quality issues and time-related aspects namely recency and change frequency Our findings are encouraging in that most of the data are structured and published in standard formats that are easy to parse there is ample opportunity to integrate different data sets and the volume of data is increasing steadily But they also uncovered a number of challenges that need to be addressed to enable these data to be fully leveraged We discuss both our findings and issues involved in using open urban data,
Big Data and Biomedical Informatics A Challenging Opportunity Summary Big data are receiving an increasing attention in biomedicine and healthcare It is therefore important to understand the reason why big data are assuming a crucial role for the biomedical informatics community The capability of handling big data is becoming an enabler to carry out unprecedented research studies and to implement new models of healthcare delivery Therefore it is first necessary to deeply understand the four elements that constitute big data namely Volume Variety Velocity and Veracity and their meaning in practice Then it is mandatory to understand where big data are present and where they can be beneficially collected There are research fields such as translational bioinformatics which need to rely on big data technologies to withstand the shock wave of data that is generated every day Other areas ranging from epidemiology to clinical care can benefit from the exploitation of the large amounts of data that are nowadays available from personal monitoring to primary care However building big data-enabled systems carries on relevant implications in terms of reproducibility of research studies and management of privacy and data access proper actions should be taken to deal with these issues An interesting consequence of the big data scenario is the availability of new software methods and tools such as map-reduce cloud computing and concept drift machine learning algorithms which will not only contribute to big data research but may be beneficial in many biomedical informatics applications The way forward with the big data opportunity will require properly applied engineering principles to design studies and applications to avoid preconceptions or over-enthusiasms to fully exploit the available technologies and to improve data processing and data management regulations,
Big Data Usage Patterns in the Health Care Domain A Use Case Driven Approach Applied to the Assessment of Vaccination Benefits and Risks Summary Background Generally benefits and risks of vaccines can be determined from studies carried out as part of regulatory compliance followed by surveillance of routine data however there are some rarer and more long term events that require new methods Big data generated by increasingly affordable personalised computing and from pervasive computing devices is rapidly growing and low cost high volume cloud computing makes the processing of these data inexpensive Objective To describe how big data and related analytical methods might be applied to assess the benefits and risks of vaccines Method We reviewed the literature on the use of big data to improve health applied to generic vaccine use cases that illustrate benefits and risks of vaccination We defined a use case as the interaction between a user and an information system to achieve a goal We used flu vaccination and pre-school childhood immunisation as exemplars Results We reviewed three big data use cases relevant to assessing vaccine benefits and risks i Big data processing using crowd-sourcing distributed big data processing and predictive analytics ii Data integration from heterogeneous big data sources eg the increasing range of devices in the internet of things and iii Real-time monitoring for the direct monitoring of epidemics as well as vaccine effects via social media and other data sources Conclusions Big data raises new ethical dilemmas though its analysis methods can bring complementary real-time capabilities for monitoring epidemics and assessing vaccine benefit-risk balance,
Big data analytics in healthcare promise and potential Objective To describe the promise and potential of big data analytics in healthcare Methods The paper describes the nascent field of big data analytics in healthcare discusses the benefits outlines an architectural framework and methodology describes examples reported in the literature briefly discusses the challenges and offers conclusions Results The paper provides a broad overview of big data analytics for healthcare researchers and practitioners Conclusions Big data analytics in healthcare is evolving into a promising field for providing insight from very large data sets and improving outcomes while reducing costs Its potential is great however there remain challenges to overcome,
Big Data and Clinicians A Review on the State of the Science Background In the past few decades medically related data collection saw a huge increase referred to as big data These huge datasets bring challenges in storage processing and analysis In clinical medicine big data is expected to play an important role in identifying causality of patient symptoms in predicting hazards of disease incidence or reoccurrence and in improving primary-care quality Objective The objective of this review was to provide an overview of the features of clinical big data describe a few commonly employed computational algorithms statistical methods and software toolkits for data manipulation and analysis and discuss the challenges and limitations in this realm Methods We conducted a literature review to identify studies on big data in medicine especially clinical medicine We used different combinations of keywords to search PubMed Science Direct Web of Knowledge and Google Scholar for literature of interest from the past  years Results This paper reviewed studies that analyzed clinical big data and discussed issues related to storage and analysis of this type of data Conclusions Big data is becoming a common feature of biological and clinical studies Researchers who use clinical big data face multiple challenges and the data itself has limitations It is imperative that methodologies for data analysis keep pace with our ability to collect and store data,
Big Data in Healthcare  Defining the Digital Persona through User Contexts from the Micro to the Macro Summary Objectives While big data offers enormous potential for improving healthcare delivery many of the existing claims concerning big data in healthcare are based on anecdotal reports and theoretical vision papers rather than scientific evidence based on empirical research Historically the implementation of health information technology has resulted in unintended consequences at the individual organizational and social levels but these unintended consequences of collecting data have remained unaddressed in the literature on big data The objective of this paper is to provide insights into big data from the perspective of people social and organizational considerations Method We draw upon the concept of persona to define the digital persona as the intersection of data tasks and context for different user groups We then describe how the digital persona can serve as a framework to understanding sociotechnical considerations of big data implementation We then discuss the digital persona in the context of micro meso and macro user groups across the  Vs of big data Results We provide insights into the potential benefits and challenges of applying big data approaches to healthcare as well as how to position these approaches to achieve health system objectives such as patient safety or patient-engaged care delivery We also provide a framework for defining the digital persona at a micro meso and macro level to help understand the user contexts of big data solutions Conclusion While big data provides great potential for improving healthcare delivery it is essential that we consider the individual social and organizational contexts of data use when implementing big data solutions,
Potentiality of Big Data in the Medical Sector Focus on How to Reshape the Healthcare System Objectives The main purpose of this study was to explore whether the use of big data can effectively reduce healthcare concerns such as the selection of appropriate treatment paths improvement of healthcare systems and so on Methods By providing an overview of the current state of big data applications in the healthcare environment this study has explored the current challenges that governments and healthcare stakeholders are facing as well as the opportunities presented by big data Results Insightful consideration of the current state of big data applications could help follower countries or healthcare stakeholders in their plans for deploying big data to resolve healthcare issues The advantage for such follower countries and healthcare stakeholders is that they can possibly leapfrog the leaders' big data applications by conducting a careful analysis of the leaders' successes and failures and exploiting the expected future opportunities in mobile services Conclusions First all big data projects undertaken by leading countries' governments and healthcare industries have similar general common goals Second for medical data that cuts across departmental boundaries a top-down approach is needed to effectively manage and integrate big data Third real-time analysis of in-motion big data should be carried out while protecting privacy and security,
What Difference Does Quantity Make On the Epistemology of Big Data in Biology Is big data science a whole new way of doing research And what difference does data quantity make to knowledge production strategies and their outputs I argue that the novelty of big data science does not lie in the sheer quantity of data involved but rather in  the prominence and status acquired by data as commodity and recognised output both within and outside of the scientific community and  the methods infrastructures technologies skills and knowledge developed to handle data These developments generate the impression that data-intensive research is a new mode of doing science with its own epistemology and norms To assess this claim one needs to consider the ways in which data are actually disseminated and used to generate knowledge Accordingly this paper reviews the development of sophisticated ways to disseminate integrate and re-use data acquired on model organisms over the last three decades of work in experimental biology I focus on online databases as prominent infrastructures set up to organise and interpret such data and examine the wealth and diversity of expertise resources and conceptual scaffolding that such databases draw upon This illuminates some of the conditions under which big data need to be curated to support processes of discovery across biological subfields which in turn highlights the difficulties caused by the lack of adequate curation for the vast majority of data in the life sciences In closing I reflect on the difference that data quantity is making to contemporary biology the methodological and epistemic challenges of identifying and analyzing data given these developments and the opportunities and worries associated to big data discourse and methods,
Big Data Smart Homes and Ambient Assisted Living Summary Objectives To discuss how current research in the area of smart homes and ambient assisted living will be influenced by the use of big data Methods A scoping review of literature published in scientific journals and conference proceedings was performed focusing on smart homes ambient assisted living and big data over the years - Results The health and social care market has lagged behind other markets when it comes to the introduction of innovative IT solutions and the market faces a number of challenges as the use of big data will increase First there is a need for a sustainable and trustful information chain where the needed information can be transferred from all producers to all consumers in a structured way Second there is a need for big data strategies and policies to manage the new situation where information is handled and transferred independently of the place of the expertise Finally there is a possibility to develop new and innovative business models for a market that supports cloud computing social media crowdsourcing etc Conclusions The interdisciplinary area of big data smart homes and ambient assisted living is no longer only of interest for IT developers it is also of interest for decision makers as customers make more informed choices among todays services In the future it will be of importance to make information usable for managers and improve decision making tailor smart home services based on big data develop new business models increase competition and identify policies to ensure privacy security and liability,
Challenges of Big Data Analysis Big Data bring new opportunities to modern society and challenges to data scientists On one hand Big Data hold great promises for discovering subtle population patterns and heterogeneities that are not possible with small-scale data On the other hand the massive sample size and high dimensionality of Big Data introduce unique computational and statistical challenges including scalability and storage bottleneck noise accumulation spurious correlation incidental endogeneity and measurement errors These challenges are distinguished and require new computational and statistical paradigm This article gives overviews on the salient features of Big Data and how these features impact on paradigm change on statistical and computational methods as well as computing architectures We also provide various new perspectives on the Big Data analysis and computation In particular we emphasize on the viability of the sparsest solution in high-confidence set and point out that exogeneous assumptions in most statistical methods for Big Data can not be validated due to incidental endogeneity They can lead to wrong statistical inferences and consequently wrong scientific conclusions,
Whats So Different about Big Data A Primer for Clinicians Trained to Think Epidemiologically The Big Data movement in computer science has brought dramatic changes in what counts as data how those data are analyzed and what can be done with those data Although increasingly pervasive in the business world it has only recently begun to influence clinical research and practice As Big Data draws from different intellectual traditions than clinical epidemiology the ideas may be less familiar to practicing clinicians There is an increasing role of Big Data in health care and it has tremendous potential This Demystifying Data Seminar identifies four main strands in Big Data relevant to health care The first is the inclusion of many new kinds of data elements into clinical research and operations in a volume not previously routinely used Second Big Data asks different kinds of questions of data and emphasizes the usefulness of analyses that are explicitly associational but not causal Third Big Data brings new analytic approaches to bear on these questions And fourth Big Data embodies a new set of aspirations for a breaking down of distinctions between research data and operational data and their merging into a continuously learning health system,
Big Data in Medicine is Driving Big Changes Summary Objectives To summarise current research that takes advantage of Big Data in health and biomedical informatics applications Methods Survey of trends in this work and exploration of literature describing how large-scale structured and unstructured data sources are being used to support applications from clinical decision making and health policy to drug design and pharmacovigilance and further to systems biology and genetics Results The survey highlights ongoing development of powerful new methods for turning that large-scale and often complex data into information that provides new insights into human health in a range of different areas Consideration of this body of work identifies several important paradigm shifts that are facilitated by Big Data resources and methods in clinical and translational research from hypothesis-driven research to data-driven research and in medicine from evidence-based practice to practice-based evidence Conclusions The increasing scale and availability of large quantities of health data require strategies for data management data linkage and data integration beyond the limits of many existing information systems and substantial effort is underway to meet those needs As our ability to make sense of that data improves the value of the data will continue to increase Health systems genetics and genomics population and public health all areas of biomedicine stand to benefit from Big Data and the associated technologies,
Applications of the MapReduce programming framework to clinical big data analysis current landscape and future trends The emergence of massive datasets in a clinical setting presents both challenges and opportunities in data storage and analysis This so called big data challenges traditional analytic tools and will increasingly require novel solutions adapted from other fields Advances in information and communication technology present the most viable solutions to big data analysis in terms of efficiency and scalability It is vital those big data solutions are multithreaded and that data access approaches be precisely tailored to large volumes of semi-structuredunstructured data The MapReduce programming framework uses two tasks common in functional programming Map and Reduce MapReduce is a new parallel processing framework and Hadoop is its open-source implementation on a single computing node or on clusters Compared with existing parallel processing paradigms eg grid computing and graphical processing unit GPU MapReduce and Hadoop have two advantages  fault-tolerant storage resulting in reliable data processing by replicating the computing tasks and cloning the data chunks on different computing nodes across the computing cluster  high-throughput data processing via a batch processing framework and the Hadoop distributed file system HDFS Data are stored in the HDFS and made available to the slave nodes for computation In this paper we review the existing applications of the MapReduce programming framework and its implementation platform Hadoop in clinical big data and related medical health informatics fields The usage of MapReduce and Hadoop on a distributed system represents a significant advance in clinical big data processing and utilization and opens up new opportunities in the emerging era of big data analytics The objective of this paper is to summarize the state-of-the-art efforts in clinical big data analytics and highlight what might be needed to enhance the outcomes of clinical big data analytics tools This paper is concluded by summarizing the potential usage of the MapReduce programming framework and Hadoop platform to process huge volumes of clinical data in medical health informatics related fields,
Big Data in Science and Healthcare A Review of Recent Literature and Perspectives Summary Objectives As technology continues to evolve and rise in various industries such as healthcare science education and gaming a sophisticated concept known as Big Data is surfacing The concept of analytics aims to understand data We set out to portray and discuss perspectives of the evolving use of Big Data in science and healthcare and to examine some of the opportunities and challenges Methods A literature review was conducted to highlight the implications associated with the use of Big Data in scientific research and healthcare innovations both on a large and small scale Results Scientists and health-care providers may learn from one another when it comes to understanding the value of Big Data and analytics Small data derived by patients and consumers also requires analytics to become actionable Connectivism provides a framework for the use of Big Data and analytics in the areas of science and healthcare This theory assists individuals to recognize and synthesize how human connections are driving the increase in data Despite the volume and velocity of Big Data it is truly about technology connecting humans and assisting them to construct knowledge in new ways Concluding Thoughts The concept of Big Data and associated analytics are to be taken seriously when approaching the use of vast volumes of both structured and unstructured data in science and health-care Future exploration of issues surrounding data privacy confidentiality and education are needed A greater focus on data from social media the quantified self-movement and the application of analytics to small data would also be useful,
Big Data Are Biomedical and Health Informatics Training Programs Ready Summary Objectives The growing volume and diversity of health and biomedical data indicate that the era of Big Data has arrived for healthcare This has many implications for informatics not only in terms of implementing and evaluating information systems but also for the work and training of informatics researchers and professionals This article addresses the question What do biomedical and health informaticians working in analytics and Big Data need to know Methods We hypothesize a set of skills that we hope will be discussed among academic and other informaticians Results The set of skills includes Programming - especially with data-oriented tools such as SQL and statistical programming languages Statistics - working knowledge to apply tools and techniques Domain knowledge - depending on ones area of work bioscience or health care and Communication - being able to understand needs of people and organizations and articulate results back to them Conclusions Biomedical and health informatics educational programs must introduce concepts of analytics Big Data and the underlying skills to use and apply them into their curricula The development of new coursework should focus on those who will become experts with training aiming to provide skills in deep analytical talent as well as those who need knowledge to support such individuals,
Trends in IT Innovation to Build a Next Generation Bioinformatics Solution to Manage and Analyse Biological Big Data Produced by NGS Technologies Sequencing the human genome began in  and  years of work were necessary in order to provide a nearly complete sequence Nowadays NGS technologies allow sequencing of a whole human genome in a few days This deluge of data challenges scientists in many ways as they are faced with data management issues and analysis and visualization drawbacks due to the limitations of current bioinformatics tools In this paper we describe how the NGS Big Data revolution changes the way of managing and analysing data We present how biologists are confronted with abundance of methods tools and data formats To overcome these problems focus on Big Data Information Technology innovations from web and business intelligence We underline the interest of NoSQL databases which are much more efficient than relational databases Since Big Data leads to the loss of interactivity with data during analysis due to high processing time we describe solutions from the Business Intelligence that allow one to regain interactivity whatever the volume of data is We illustrate this point with a focus on the Amadea platform Finally we discuss visualization challenges posed by Big Data and present the latest innovations with JavaScript graphic libraries,
Big Data Analytics in Healthcare The rapidly expanding field of big data analytics has started to play a pivotal role in the evolution of healthcare practices and research It has provided tools to accumulate manage analyze and assimilate large volumes of disparate structured and unstructured data produced by current healthcare systems Big data analytics has been recently applied towards aiding the process of care delivery and disease exploration However the adoption rate and research development in this space is still hindered by some fundamental problems inherent within the big data paradigm In this paper we discuss some of these major challenges with a focus on three upcoming and promising areas of medical research image signal and genomics based analytics Recent research which targets utilization of large volumes of medical data while combining multimodal data from disparate sources is discussed Potential areas of research within this field which have the ability to provide meaningful impact on healthcare delivery are also examined,
Managing Analysing and Integrating Big Data in Medical Bioinformatics Open Problems and Future Perspectives The explosion of the data both in the biomedical research and in the healthcare systems demands urgent solutions In particular the research in omics sciences is moving from a hypothesis-driven to a data-driven approach Healthcare is additionally always asking for a tighter integration with biomedical data in order to promote personalized medicine and to provide better treatments Efficient analysis and interpretation of Big Data opens new avenues to explore molecular biology new questions to ask about physiological and pathological states and new ways to answer these open issues Such analyses lead to better understanding of diseases and development of better and personalized diagnostics and therapeutics However such progresses are directly related to the availability of new solutions to deal with this huge amount of information New paradigms are needed to store and access data for its annotation and integration and finally for inferring knowledge and making it available to researchers Bioinformatics can be viewed as the glue for all these processes A clear awareness of present high performance computing HPC solutions in bioinformatics Big Data analysis paradigms for computational biology and the issues that are still open in the biomedical and healthcare fields represent the starting point to win this challenge,
Some experiences and opportunities for big data in translational research Health care has become increasingly information intensive The advent of genomic data integrated into patient care significantly accelerates the complexity and amount of clinical data Translational research in the present day increasingly embraces new biomedical discovery in this data-intensive world thus entering the domain of big data The Electronic Medical Records and Genomics consortium has taught us many lessons while simultaneously advances in commodity computing methods enable the academic community to affordably manage and process big data Although great promise can emerge from the adoption of big data methods and philosophy the heterogeneity and complexity of clinical data in particular pose additional challenges for big data inferencing and clinical application However the ultimate comparability and consistency of heterogeneous clinical information sources can be enhanced by existing and emerging data standards which promise to bring order to clinical data chaos Meaningful Use data standards in particular have already simplified the task of identifying clinical phenotyping patterns in electronic health records,
Big Data Astronomical or Genomical Genomics is a Big Data science and is going to get much bigger very soon but it is not known whether the needs of genomics will exceed other Big Data domains Projecting to the year  we compared genomics with three other major generators of Big Data astronomy YouTube and Twitter Our estimates show that genomics is a four-headed beastit is either on par with or the most demanding of the domains analyzed here in terms of data acquisition storage distribution and analysis We discuss aspects of new technologies that will need to be developed to rise up and meet the computational challenges that genomics poses for the near future Now is the time for concerted community-wide planning for the genomical challenges of the next decade,
Empowering Personalized Medicine with Big Data and Semantic Web Technology Promises Challenges and Use Cases In healthcare big data tools and technologies have the potential to create significant value by improving outcomes while lowering costs for each individual patient Diagnostic images genetic test results and biometric information are increasingly generated and stored in electronic health records presenting us with challenges in data that is by nature high volume variety and velocity thereby necessitating novel ways to store manage and process big data This presents an urgent need to develop new scalable and expandable big data infrastructure and analytical methods that can enable healthcare providers access knowledge for the individual patient yielding better decisions and outcomes In this paper we briefly discuss the nature of big data and the role of semantic web and data analysis for generating smart data which offer actionable information that supports better decision for personalized medicine In our view the biggest challenge is to create a system that makes big data robust and smart for healthcare providers and patients that can lead to more effective clinical decision-making improved health outcomes and ultimately managing the healthcare costs We highlight some of the challenges in using big data and propose the need for a semantic data-driven environment to address them We illustrate our vision with practical use cases and discuss a path for empowering personalized medicine using big data and semantic web technology,
Big Data and Ambulatory Care Big data is heralded as having the potential to revolutionize health care by making large amounts of data available to support care delivery population health and patient engagement Critics argue that big data's transformative potential is inhibited by privacy requirements that restrict health information exchange However there are a variety of permissible activities involving use and disclosure of patient information that support care delivery and management This article presents an overview of the legal framework governing health information dispels misconceptions about privacy regulations and highlights how ambulatory care providers in particular can maximize the utility of big data to improve care,
Big data - a st century science Maginot Line No-boundary thinking shifting from the big data paradigm Whether your interests lie in scientific arenas the corporate world or in government you have certainly heard the praises of big data Big data will give you new insights allow you to become more efficient andor will solve your problems While big data has had some outstanding successes many are now beginning to see that it is not the Silver Bullet that it has been touted to be Here our main concern is the overall impact of big data the current manifestation of big data is constructing a Maginot Line in science in the st century Big data is not lots of data as a phenomena anymore The big data paradigm is putting the spirit of the Maginot Line into lots of data Big data overall is disconnecting researchers and science challenges We propose No-Boundary Thinking NBT applying no-boundary thinking in problem defining to address science challenges,
Challenges and Potential Solutions for Big Data Implementations in Developing Countries Summary Background The volume of data the velocity with which they are generated and their variety and lack of structure hinder their use This creates the need to change the way information is captured stored processed and analyzed leading to the paradigm shift called Big Data Objectives To describe the challenges and possible solutions for developing countries when implementing Big Data projects in the health sector Methods A non-systematic review of the literature was performed in PubMed and Google Scholar The following keywords were used big data developing countries data mining health information systems and computing methodologies A thematic review of selected articles was performed Results There are challenges when implementing any Big Data program including exponential growth of data special infrastructure needs need for a trained workforce need to agree on interoperability standards privacy and security issues and the need to include people processes and policies to ensure their adoption Developing countries have particular characteristics that hinder further development of these projects Conclusions The advent of Big Data promises great opportunities for the healthcare field In this article we attempt to describe the challenges developing countries would face and enumerate the options to be used to achieve successful implementations of Big Data programs,
Big Data in Medical Sciencea Biostatistical View Background Inexpensive techniques for measurement and data storage now enable medical researchers to acquire far more data than can conveniently be analyzed by traditional methods The expression big data refers to quantities on the order of magnitude of a terabyte  bytes special techniques must be used to evaluate such huge quantities of data in a scientifically meaningful way Whether data sets of this size are useful and important is an open question that currently confronts medical science Methods In this article we give illustrative examples of the use of analytical techniques for big data and discuss them in the light of a selective literature review We point out some critical aspects that should be considered to avoid errors when large amounts of data are analyzed Results Machine learning techniques enable the recognition of potentially relevant patterns When such techniques are used certain additional steps should be taken that are unnecessary in more traditional analyses for example patient characteristics should be differentially weighted If this is not done as a preliminary step before similarity detection which is a component of many data analysis operations characteristics such as age or sex will be weighted no higher than any one out of   gene expression values Experience from the analysis of conventional observational data sets can be called upon to draw conclusions about potential causal effects from big data sets Conclusion Big data techniques can be used for example to evaluate observational data derived from the routine care of entire populations with clustering methods used to analyze therapeutically relevant patient subgroups Such analyses can provide complementary information to clinical trials of the classic type As big data analyses become more popular various statistical techniques for causality analysis in observational data are becoming more widely available This is likely to be of benefit to medical science but specific adaptations will have to be made according to the requirements of the applications,
Big Data - Smart Health Strategies Summary Objectives To select best papers published in  in the field of big data and smart health strategies and summarize outstanding research efforts Methods A systematic search was performed using two major bibliographic databases for relevant journal papers The references obtained were reviewed in a two-stage process starting with a blinded review performed by the two section editors and followed by a peer review process operated by external reviewers recognized as experts in the field Results The complete review process selected four best papers illustrating various aspects of the special theme among them a using large volumes of unstructured data and specifically clinical notes from Electronic Health Records EHRs for pharmacovigilance b knowledge discovery via querying large volumes of complex both structured and unstructured biological data using big data technologies and relevant tools c methodologies for applying cloud computing and big data technologies in the field of genomics and d system architectures enabling high-performance access to and processing of large datasets extracted from EHRs Conclusions The potential of big data in biomedicine has been pinpointed in various viewpoint papers and editorials The review of current scientific literature illustrated a variety of interesting methods and applications in the field but still the promises exceed the current outcomes As we are getting closer towards a solid foundation with respect to common understanding of relevant concepts and technical aspects and the use of standardized technologies and tools we can anticipate to reach the potential that big data offer for personalized medicine and smart health strategies in the near future,
Big data the next frontier for innovation in therapeutics and healthcare Advancements in genomics and personalized medicine not only effect healthcare delivery from patient and provider standpoints but also reshape biomedical discovery We are in the era of the -omics wherein an individuals genome transcriptome proteome and metabolome can be scrutinized to the finest resolution to paint a personalized biochemical fingerprint that enables tailored treatments prognoses risk factors etc Digitization of this information parlays into big data informatics-driven evidence-based medical practice While individualized patient management is a key beneficiary of next-generation medical informatics this data also harbors a wealth of novel therapeutic discoveries waiting to be uncovered Big data informatics allows for networks-driven systems pharmacodynamics whereby drug information can be coupled to cellular- and organ-level physiology for determining whole-body outcomes Patient -omics data can be integrated for ontology-based data-mining for the discovery of new biological associations and drug targets Here we highlight the potential of big data informatics for clinical pharmacology,
What Does Big Data Mean for Wearable Sensor Systems Summary Objectives The aim of this paper is to discuss how recent developments in the field of big data may potentially impact the future use of wearable sensor systems in healthcare Methods The article draws on the scientific literature to support the opinions presented by the IMIA Wearable Sensors in Healthcare Working Group Results The following is discussed the potential for wearable sensors to generate big data how complementary technologies such as a smartphone will augment the concept of a wearable sensor and alter the nature of the monitoring data created how standards would enable sharing of data and advance scientific progress Importantly attention is drawn to statistical inference problems for which big datasets provide little assistance or may hinder the identification of a useful solution Finally a discussion is presented on risks to privacy and possible negative consequences arising from intensive wearable sensor monitoring Conclusions Wearable sensors systems have the potential to generate datasets which are currently beyond our capabilities to easily organize and interpret In order to successfully utilize wearable sensor data to infer wellbeing and enable proactive health management standards and ontologies must be developed which allow for data to be shared between research groups and between commercial systems promoting the integration of these data into health information systems However policy and regulation will be required to ensure that the detailed nature of wearable sensor data is not misused to invade privacies or prejudice against individuals,
Big data in global health improving health in low- and middle-income countries Abstract Over the last decade a massive increase in data collection and analysis has occurred in many fields In the health sector however there has been relatively little progress in data analysis and application despite a rapid rise in data production Given adequate governance improvements in the quality quantity storage and analysis of health data could lead to substantial improvements in many health outcomes In low- and middle-income countries in particular the creation of an information feedback mechanism can move health-care delivery towards results-based practice and improve the effective use of scarce resources We review the evolving definition of big data and the possible advantages of  and problems in  using such data to improve health-care delivery in low- and middle-income countries The collection of big data as mobile-phone based services improve may mean that development phases required elsewhere can be skipped However poor infrastructure may prevent interoperability and the safe use of patient data An appropriate governance framework must be developed and enforced to protect individuals and ensure that health-care delivery is tailored to the characteristics and values of the target communities,
Visual analytics in healthcare education exploring novel ways to analyze and represent big data in undergraduate medical education Introduction The big data present in the medical curriculum that informs undergraduate medical education is beyond human abilities to perceive and analyze The medical curriculum is the main tool used by teachers and directors to plan design and deliver teaching and assessment activities and student evaluations in medical education in a continuous effort to improve it Big data remains largely unexploited for medical education improvement purposes The emerging research field of visual analytics has the advantage of combining data analysis and manipulation techniques information and knowledge representation and human cognitive strength to perceive and recognize visual patterns Nevertheless there is a lack of research on the use and benefits of visual analytics in medical education Methods The present study is based on analyzing the data in the medical curriculum of an undergraduate medical program as it concerns teaching activities assessment methods and learning outcomes in order to explore visual analytics as a tool for finding ways of representing big data from undergraduate medical education for improvement purposes Cytoscape software was employed to build networks of the identified aspects and visualize them Results After the analysis of the curriculum data eleven aspects were identified Further analysis and visualization of the identified aspects with Cytoscape resulted in building an abstract model of the examined data that presented three different approaches i learning outcomes and teaching methods ii examination and learning outcomes and iii teaching methods learning outcomes examination results and gap analysis Discussion This study identified aspects of medical curriculum that play an important role in how medical education is conducted The implementation of visual analytics revealed three novel ways of representing big data in the undergraduate medical education context It appears to be a useful tool to explore such data with possible future implications on healthcare education It also opens a new direction in medical education informatics research,
Big Data and the Electronic Health Record Summary Objectives Implementation of Electronic Health Record EHR systems continues to expand The massive number of patient encounters results in high amounts of stored data Transforming clinical data into knowledge to improve patient care has been the goal of biomedical informatics professionals for many decades and this work is now increasingly recognized outside our field In reviewing the literature for the past three years we focus on big data in the context of EHR systems and we report on some examples of how secondary use of data has been put into practice Methods We searched PubMed database for articles from January   to November   We initiated the search with keywords related to big data and EHR We identified relevant articles and additional keywords from the retrieved articles were added Based on the new keywords more articles were retrieved and we manually narrowed down the set utilizing predefined inclusion and exclusion criteria Results Our final review includes articles categorized into the themes of data mining pharmacovigilance phenotyping natural language processing data application and integration clinical decision support personal monitoring social media and privacy and security Conclusion The increasing adoption of EHR systems worldwide makes it possible to capture large amounts of clinical data There is an increasing number of articles addressing the theme of big data and the concepts associated with these articles vary The next step is to transform healthcare big data into actionable knowledge,
A survey on platforms for big data analytics The primary purpose of this paper is to provide an in-depth analysis of different platforms available for performing big data analytics This paper surveys different hardware platforms available for big data analytics and assesses the advantages and drawbacks of each of these platforms based on various metrics such as scalability data IO rate fault tolerance real-time processing data size supported and iterative task support In addition to the hardware a detailed description of the software frameworks used within each of these platforms is also discussed along with their strengths and drawbacks Some of the critical characteristics described here can potentially aid the readers in making an informed decision about the right choice of platforms depending on their computational needs Using a star ratings table a rigorous qualitative comparison between different platforms is also discussed for each of the six characteristics that are critical for the algorithms of big data analytics In order to provide more insights into the effectiveness of each of the platform in the context of big data analytics specific implementation level details of the widely used k-means clustering algorithm on various platforms are also described in the form pseudocode,
Making Big Data Useful for Health Care A Summary of the Inaugural MIT Critical Data Conference With growing concerns that big data will only augment the problem of unreliable research the Laboratory of Computational Physiology at the Massachusetts Institute of Technology organized the Critical Data Conference in January  Thought leaders from academia government and industry across disciplinesincluding clinical medicine computer science public health informatics biomedical research health technology statistics and epidemiologygathered and discussed the pitfalls and challenges of big data in health care The key message from the conference is that the value of large amounts of data hinges on the ability of researchers to share data methodologies and findings in an open setting If empirical value is to be from the analysis of retrospective data groups must continuously work together on similar problems to create more effective peer review This will lead to improvement in methodology and quality with each iteration of analysis resulting in more reliability,
Bioinformatics clouds for big data manipulation Abstract As advances in life sciences and information technology bring profound influences on bioinformatics due to its interdisciplinary nature bioinformatics is experiencing a new leap-forward from in-house computing infrastructure into utility-supplied cloud computing delivered over the Internet in order to handle the vast quantities of biological data generated by high-throughput experimental technologies Albeit relatively new cloud computing promises to address big data storage and analysis issues in the bioinformatics field Here we review extant cloud-based services in bioinformatics classify them into Data as a Service DaaS Software as a Service SaaS Platform as a Service PaaS and Infrastructure as a Service IaaS and present our perspectives on the adoption of cloud computing in bioinformatics Reviewers This article was reviewed by Frank Eisenhaber Igor Zhulin and Sandor Pongor,
Human Neuroimaging as a Big Data Science The maturation of in vivo neuroimaging has lead to incredible quantities of digital information about the human brain While much is made of the data deluge in science neuroimaging represents the leading edge of this onslaught of big data A range of neuroimaging databasing approaches has streamlined the transmission storage and dissemination of data from such brain imaging studies Yet few if any common solutions exist to support the science of neuroimaging In this article we discuss how modern neuroimaging research represents a mutifactorial and broad ranging data challenge involving the growing size of the data being acquired sociologial and logistical sharing issues infrastructural challenges for multi-site multi-datatype archiving and the means by which to explore and mine these data As neuroimaging advances further eg aging genetics and age-related disease new vision is needed to manage and process this information while marshalling of these resources into novel results Thus big data can become big brain science,
From Big Data to Knowledge in the Social Sciences One of the challenges associated with high-volume diverse datasets is whether synthesis of open data streams can translate into actionable knowledge Recognizing that challenge and other issues related to these types of data the National Institutes of Health developed the Big Data to Knowledge or BDK initiative The concept of translating big data to knowledge is important to the social and behavioral sciences in several respects First a general shift to data-intensive science will exert an influence on all scientific disciplines but particularly on the behavioral and social sciences given the wealth of behavior and related constructs captured by big data sources Second science is itself a social enterprise by applying principles from the social sciences to the conduct of research it should be possible to ameliorate some of the systemic problems that plague the scientific enterprise in the age of big data We explore the feasibility of recalibrating the basic mechanisms of the scientific enterprise so that they are more transparent and cumulative more integrative and cohesive and more rapid relevant and responsive,
An Efficient Approach for Web Indexing of Big Data through Hyperlinks in Web Crawling Web Crawling has acquired tremendous significance in recent times and it is aptly associated with the substantial development of the World Wide Web Web Search Engines face new challenges due to the availability of vast amounts of web documents thus making the retrieved results less applicable to the analysers However recently Web Crawling solely focuses on obtaining the links of the corresponding documents Today there exist various algorithms and software which are used to crawl links from the web which has to be further processed for future use thereby increasing the overload of the analyser This paper concentrates on crawling the links and retrieving all information associated with them to facilitate easy processing for other uses In this paper firstly the links are crawled from the specified uniform resource locator URL using a modified version of Depth First Search Algorithm which allows for complete hierarchical scanning of corresponding web links The links are then accessed via the source code and its metadata such as title keywords and description are extracted This content is very essential for any type of analyser work to be carried on the Big Data obtained as a result of Web Crawling,
Psychological and Social Factors Affecting Internet Searches on Suicide in Korea A Big Data Analysis of Google Search Trends Purpose The average mortality rate for death by suicide among OECD countries is  per  and  for Korea The present study analyzed big data extracted from Google to identify factors related to searches on suicide in Korea Materials and Methods Google search trends for the search words of suicide stress exercise and drinking were obtained for - Analyzing data by month the relationship between the actual number of suicides and search words per year was examined using multi-level models Results Both suicide rates and Google searches on suicide in Korea increased since  An unconditional slope model indicated stress and suicide-related searches were positively related A conditional model showed that factors associated with suicide by year directly affected suicide-related searches The interaction between stress-related searches and the actual number of suicides was significant Conclusion A positive relationship between stress- and suicide-related searches further confirmed that stress affects suicide Taken together and viewed in context of the big data analysis our results point to the need for a tailored prevention program Real-time big data can be of use in indicating increases in suicidality when search words such as stress and suicide generate greater numbers of hits on portals and social network sites,
Survey Translational Bioinformatics embraces Big Data Summary We review the latest trends and major developments in translational bioinformatics in the year  Our emphasis is on highlighting the key events in the field and pointing at promising research areas for the future The key take-home points are Translational informatics is ready to revolutionize human health and healthcare using large-scale measurements on individualsDatacentric approaches that compute on massive amounts of data often called Big Data to discover patterns and to make clinically relevant predictions will gain adoptionResearch that bridges the latest multimodal measurement technologies with large amounts of electronic healthcare data is increasing and is where new breakthroughs will occur,
Big data and clinical research focusing on the area of critical care medicine in mainland China Big data has long been found its way into clinical practice since the advent of information technology era Medical records and follow-up data can be more efficiently stored and extracted with information technology Immediately after admission a patient immediately produces a large amount of data including laboratory findings medications fluid balance progressing notes and imaging findings Clinicians and clinical investigators should make every effort to make full use of the big data that is being continuously generated by electronic medical record EMR system and other healthcare databases At this stage more training courses on data management and statistical analysis are required before clinicians and clinical investigators can handle big data and translate them into advances in medical science China is a large country with a population of  billion and can contribute greatly to clinical researches by providing reliable and high-quality big data,
Technical Challenges for Big Data in Biomedicine and Health Data SourcesInfrastructure and Analytics Summary Objectives To review technical and methodological challenges for big data research in biomedicine and health Methods We discuss sources of big datasets survey infrastructures for big data storage and big data processing and describe the main challenges that arise when analyzing big data Results The life and biomedical sciences are massively contributing to the big data revolution through secondary use of data that were collected during routine care and through new data sources such as social media Efficient processing of big datasets is typically achieved by distributing computation over a cluster of computers Data analysts should be aware of pitfalls related to big data such as bias in routine care data and the risk of false-positive findings in high-dimensional datasets Conclusions The major challenge for the near future is to transform analytical methods that are used in the biomedical and health domain to fit the distributed storage and processing model that is required to handle big data while ensuring confidentiality of the data being analyzed,
Analyzing Big Data with the Hybrid Interval Regression Methods Big data is a new trend at present forcing the significant impacts on information technologies In big data applications one of the most concerned issues is dealing with large-scale data sets that often require computation resources provided by public cloud services How to analyze big data efficiently becomes a big challenge In this paper we collaborate interval regression with the smooth support vector machine SSVM to analyze big data Recently the smooth support vector machine SSVM was proposed as an alternative of the standard SVM that has been proved more efficient than the traditional SVM in processing large-scale data In addition the soft margin method is proposed to modify the excursion of separation margin and to be effective in the gray zone that the distribution of data becomes hard to be described and the separation margin between classes,
EHR Big Data Deep Phenotyping Summary Objectives Given the quickening speed of discovery of variant disease drivers from combined patient genotype and phenotype data the objective is to provide methodology using big data technology to support the definition of deep phenotypes in medical records Methods As the vast stores of genomic information increase with next generation sequencing the importance of deep phenotyping increases The growth of genomic data and adoption of Electronic Health Records EHR in medicine provides a unique opportunity to integrate phenotype and genotype data into medical records The method by which collections of clinical findings and other health related data are leveraged to form meaningful phenotypes is an active area of research Longitudinal data stored in EHRs provide a wealth of information that can be used to construct phenotypes of patients We focus on a practical problem around data integration for deep phenotype identification within EHR data The use of big data approaches are described that enable scalable markup of EHR events that can be used for semantic and temporal similarity analysis to support the identification of phenotype and genotype relationships Conclusions Stead and colleagues  concept of using light standards to increase the productivity of software systems by riding on the wave of hardwareprocessing power is described as a harbinger for designing future healthcare systems The big data solution using flexible markup provides a route to improved utilization of processing power for organizing patient records in genotype and phenotype research,
What Role for Law Human Rights and Bioethics in an Age of Big Data Consortia Science and Consortia Ethics The Importance of Trustworthiness The global bioeconomy is generating new paradigm-shifting practices of knowledge co-production such as collective innovation large-scale data-driven global consortia science Big Science and consortia ethics Big Ethics These bioeconomic and sociotechnical practices can be forces for progressive social change but they can also raise predicaments at the interface of law human rights and bioethics In this article we examine one such double-edged practice the growing multivariate exploitation of Big Data in the health sector particularly by the private sector Commercial exploitation of health data for knowledge-based products is a key aspect of the bioeconomy and is also a topic of concern among publics around the world It is exacerbated in the current age of globally interconnected consortia science and consortia ethics which is characterized by accumulating epistemic proximity diminished academic independence extreme centrism and conflictedcompeting interests among innovation actors Extreme centrism is of particular importance as a new ideology emerging from consortia science and consortia ethics this relates to invariably taking a middle-of-the-road populist stance even in the event of human rights breaches so as to sustain the populist support needed for consortia building and collective innovation What role do law human rights and bioethicsseparate and togetherhave to play in addressing these predicaments and opportunities in early st century science and society One answer we propose is an intertwined ethico-legal normative construct namely trustworthiness By considering trustworthiness as a central pillar at the intersection of law human rights and bioethics we enable others to trust us which in turns allows different actors both nonprofit and for-profit to operate more justly in consortia science and ethics as well as to access and responsibly use health data for public benefit,
Systems Biology in the Context of Big Data and Networks Science is going through two rapidly changing phenomena one is the increasing capabilities of the computers and software tools from terabytes to petabytes and beyond and the other is the advancement in high-throughput molecular biology producing piles of data related to genomes transcriptomes proteomes metabolomes interactomes and so on Biology has become a data intensive science and as a consequence biology and computer science have become complementary to each other bridged by other branches of science such as statistics mathematics physics and chemistry The combination of versatile knowledge has caused the advent of big-data biology network biology and other new branches of biology Network biology for instance facilitates the system-level understanding of the cell or cellular components and subprocesses It is often also referred to as systems biology The purpose of this field is to understand organisms or cells as a whole at various levels of functions and mechanisms Systems biology is now facing the challenges of analyzing big molecular biological data and huge biological networks This review gives an overview of the progress in big-data biology and data handling and also introduces some applications of networks and multivariate analysis in systems biology,
PS- Big Data Data Science and You BackgroundAims Lately the popular press is rife with tantalizing references to coming advances brought by Big Data methods and software Modern living--mobile and social computing particularly--emits enormous plumes of data This data we are told can be analyzed in real-time to spot trends and yield important insights to the benefit of business and mankind generally The general idea of incidentally-produced data that can be exploited to produce valuable insights is one an HMORN audience is eminently comfortable with--it describes quite a lot of our research But where do we fit in with these new trends Have we all been Data Scientists doing Big Data for years now and the rest of the world is just now catching up to us Or are these things really different and new Are there things we should be appropriating from this new field to make our own work stronger Methods The proposed talk will describe and define several commonly-cited ideas and methods--to wit a big data b mapreduce c no SQL and d data science Results The talk will locate these in a larger technological context list synonyms and closely related technologies and describe situations where expanding into less-familiar tools may well bear fruit for research data projects Conclusions While much of the tools and methods of big data are squarely addressed to problems we dont frequently encounter in HMORN research it is good to have a conceptual understanding of them so that when we do hit the limits of conventional methods and resources we have someplace to go before giving a project up as infeasible,
Big Data in Pharmaceutical RD Creating a Sustainable RD Engine Over the last years productivity in the pharmaceutical industry has been diminishing because of constantly increasing costs while output has overall been stagnant Despite many efforts productivity remains a challenge within the industry At the same time healthcare providers quite rightly require better value for money and clear evidence that new drugs are better than the current standard of care making a complex situation even more complex With the implementation of Big Data initiatives trying to integrate data from disparate data sources and disciplines that are available in life science the industry has identified a new frontier that might provide the insights needed to turn the ship around and allow the industry to return to sustainable growth,
The Challenge of Big Data in Public Health An Opportunity for Visual Analytics Public health PH data can generally be characterized as big data The efficient and effective use of this data determines the extent to which PH stakeholders can sufficiently address societal health concerns as they engage in a variety of work activities As stakeholders interact with data they engage in various cognitive activities such as analytical reasoning decision-making interpreting and problem solving Performing these activities with big data is a challenge for the unaided mind as stakeholders encounter obstacles relating to the datas volume variety velocity and veracity Such being the case computer-based information tools are needed to support PH stakeholders Unfortunately while existing computational tools are beneficial in addressing certain work activities they fall short in supporting cognitive activities that involve working with large heterogeneous and complex bodies of data This paper presents visual analytics VA tools a nascent category of computational tools that integrate data analytics with interactive visualizations to facilitate the performance of cognitive activities involving big data Historically PH has lagged behind other sectors in embracing new computational technology In this paper we discuss the role that VA tools can play in addressing the challenges presented by big data In doing so we demonstrate the potential benefit of incorporating VA tools into PH practice in addition to highlighting the need for further systematic and focused research,
Genomic Sequencing Assessing The Health Care System Policy And Big-Data Implications New genomic sequencing technologies enable the high-speed analysis of multiple genes simultaneously including all of those in a person's genome Sequencing is a prominent example of a big data technology because of the massive amount of information it produces and its complexity diversity and timeliness Our objective in this article is to provide a policy primer on sequencing and illustrate how it can affect health care system and policy issues Toward this end we developed an easily applied classification of sequencing based on inputs methods and outputs We used it to examine the implications of sequencing for three health care system and policy issues making care more patient-centered developing coverage and reimbursement policies and assessing economic value We conclude that sequencing has great promise but that policy challenges include how to optimize patient engagement as well as privacy develop coverage policies that distinguish research from clinical uses and account for bioinformatics costs and determine the economic value of sequencing through complex economic models that take into account multiple findings and downstream costs,
Data Mining Methods for Omics and Knowledge of Crude Medicinal Plants toward Big Data Biology Molecular biological data has rapidly increased with the recent progress of the Omics fields eg genomics transcriptomics proteomics and metabolomics that necessitates the development of databases and methods for efficient storage retrieval integration and analysis of massive data The present study reviews the usage of KNApSAcK Family DB in metabolomics and related area discusses several statistical methods for handling multivariate data and shows their application on Indonesian blended herbal medicines Jamu as a case study Exploration using Biplot reveals many plants are rarely utilized while some plants are highly utilized toward specific efficacy Furthermore the ingredients of Jamu formulas are modeled using Partial Least Squares Discriminant Analysis PLS-DA in order to predict their efficacy The plants used in each Jamu medicine served as the predictors whereas the efficacy of each Jamu provided the responses This model produces  correct classification in predicting efficacy Permutation test then is used to determine plants that serve as main ingredients in Jamu formula by evaluating the significance of the PLS-DA coefficients Next in order to explain the role of plants that serve as main ingredients in Jamu medicines information of pharmacological activity of the plants is added to the predictor block Then N-PLS-DA model multiway version of PLS-DA is utilized to handle the three-dimensional array of the predictor block The resulting N-PLS-DA model reveals that the effects of some pharmacological activities are specific for certain efficacy and the other activities are diverse toward many efficacies Mathematical modeling introduced in the present study can be utilized in global analysis of big data targeting to reveal the underlying biology,
From big data analysis to personalized medicine for all challenges and opportunities Recent advances in high-throughput technologies have led to the emergence of systems biology as a holistic science to achieve more precise modeling of complex diseases Many predict the emergence of personalized medicine in the near future We are however moving from two-tiered health systems to a two-tiered personalized medicine Omics facilities are restricted to affluent regions and personalized medicine is likely to widen the growing gap in health systems between high and low-income countries This is mirrored by an increasing lag between our ability to generate and analyze big data Several bottlenecks slow-down the transition from conventional to personalized medicine generation of cost-effective high-throughput data hybrid education and multidisciplinary teams data storage and processing data integration and interpretation and individual and global economic relevance This review provides an update of important developments in the analysis of big data and forward strategies to accelerate the global transition to personalized medicine,
Sensor Signal and Imaging Informatics Big Data and Smart Health Technologies Summary Objectives This synopsis presents a selection for the IMIA International Medical Informatics Association Yearbook  of excellent research in the broad field of Sensor Signal and Imaging Informatics published in the year  with a focus on Big Data and Smart Health Technologies Methods We performed a systematic initial selection and a double blind peer review process to find the best papers in this domain published in  from the PubMed and Web of Science databases A set of MeSH keywords provided by experts was used Results Big Data are collections of large and complex datasets which have the potential to capture the whole variability of a study population More and more innovative sensors are emerging allowing to enrich these big databases However they become more and more challenging to process ie capture store search share transfer exploit because traditional tools are not adapted anymore Conclusions This review shows that it is necessary not only to develop new tools specifically designed for Big Data but also to evaluate their performance on such large datasets,
 a small bacteria and a big data world Objective To describe the importance of bioinformatics tools to analyze the big data yielded from new omics generation-methods with the aim of unraveling the biology of the pathogen bacteria Lactococcus garvieae Methods The paper provides the vision of the large volume of data generated from genome sequences gene expression profiles by microarrays and other experimental methods that require biomedical informatics methods for management and analysis Results The use of biomedical informatics methods improves the analysis of big data in order to obtain a comprehensive characterization and understanding of the biology of pathogenic organisms such as L garvieae Conclusions The Big Data concepts of high volume veracity and variety are nowadays part of the research in microbiology associated with the use of multiple methods in the omic era The use of biomedical informatics methods is a requisite necessary to improve the analysis of these data,
Mobile Cloud and Big Data Computing Contributions Challenges and New Directions in Telecardiology Many studies have indicated that computing technology can enable off-site cardiologists to read patients electrocardiograph ECG echocardiography ECHO and relevant images via smart phones during pre-hospital in-hospital and post-hospital teleconsultation which not only identifies emergency cases in need of immediate treatment but also prevents the unnecessary re-hospitalizations Meanwhile several studies have combined cloud computing and mobile computing to facilitate better storage delivery retrieval and management of medical files for telecardiology In the future the aggregated ECG and images from hospitals worldwide will become big data which should be used to develop an e-consultation program helping on-site practitioners deliver appropriate treatment With information technology real-time tele-consultation and tele-diagnosis of ECG and images can be practiced via an e-platform for clinical research and educational purposes While being devoted to promote the application of information technology onto telecardiology we need to resolve several issues  data confidentiality in the cloud  data interoperability among hospitals and  network latency and accessibility If these challenges are overcome tele-consultation will be ubiquitous easy to perform inexpensive and beneficial Most importantly these services will increase global collaboration and advance clinical practice education and scientific research in cardiology,
Big data open science and the brain lessons learned from genomics The BRAIN Initiative aims to break new ground in the scale and speed of data collection in neuroscience requiring tools to handle data in the magnitude of yottabytes  The scale investment and organization of it are being compared to the Human Genome Project HGP which has exemplified big science for biology In line with the trend towards Big Data in genomic research the promise of the BRAIN Initiative as well as the European Human Brain Project rests on the possibility to amass vast quantities of data to model the complex interactions between the brain and behavior and inform the diagnosis and prevention of neurological disorders and psychiatric disease Advocates of this data driven paradigm in neuroscience argue that harnessing the large quantities of data generated across laboratories worldwide has numerous methodological ethical and economic advantages but it requires the neuroscience community to adopt a culture of data sharing and open access to benefit from them In this article we examine the rationale for data sharing among advocates and briefly exemplify these in terms of new open neuroscience projects Then drawing on the frequently invoked model of data sharing in genomics we go on to demonstrate the complexities of data sharing shedding light on the sociological and ethical challenges within the realms of institutions researchers and participants namely dilemmas around publicprivate interests in data lack of motivation to share in the academic community and potential loss of participant anonymity Our paper serves to highlight some foreseeable tensions around data sharing relevant to the emergent open neuroscience movement,
Big data in wildlife research remote web-based monitoring of hibernating black bears Background Numerous innovations for the management and collection of big data have arisen in the field of medicine including implantable computers and sensors wireless data transmission and web-based repositories for collecting and organizing information Recently human clinical devices have been deployed in captive and free-ranging wildlife to aid in the characterization of both normal physiology and the interaction of animals with their environment including reactions to humans Although these devices have had a significant impact on the types and quantities of information that can be collected their utility has been limited by internal memory capacities the efforts required to extract and analyze information and by the necessity to handle the animals in order to retrieve stored data Results We surgically implanted miniaturized cardiac monitors cc Reveal LINQ Medtronic Inc a newly developed human clinical system into hibernating wild American black bears N These devices include wireless capabilities which enabled frequent transmissions of detailed physiological data from bears in their remote den sites to a web-based data storage and management system Solar and battery powered telemetry stations transmitted detailed physiological data over the cellular network during the winter months The system provided the transfer of large quantities of data in near-real time Observations included changes in heart rhythms associated with birthing and caring for cubs and in all bears long periods without heart beats up to seconds occurred during each respiratory cycle Conclusions For the first time detailed physiological data were successfully transferred from an animal in the wild to a web-based data collection and management system overcoming previous limitations on the quantities of data that could be transferred The system provides an opportunity to detect unusual events as they are occurring enabling investigation of the animal and site shortly afterwards Although the current study was limited to bears in winter dens we anticipate that future systems will transmit data from implantable monitors to wearable transmitters allowing for big data transfer on non-stationary animals,
Characterization and identification of ubiquitin conjugation sites with E ligase recognition specificities Background In eukaryotes ubiquitin-conjugation is an important mechanism underlying proteasome-mediated degradation of proteins and as such plays an essential role in the regulation of many cellular processes In the ubiquitin-proteasome pathway E ligases play important roles by recognizing a specific protein substrate and catalyzing the attachment of ubiquitin to a lysine K residue As more and more experimental data on ubiquitin conjugation sites become available it becomes possible to develop prediction models that can be scaled to big data However no development that focuses on the investigation of ubiquitinated substrate specificities has existed Herein we present an approach that exploits an iteratively statistical method to identify ubiquitin conjugation sites with substrate site specificities Results In this investigation totally  experimentally validated ubiquitinated proteins were obtained from dbPTM After having filtered out homologous fragments with  sequence identity the training data set contained  ubiquitination sites positive data and  non-ubiquitinated sites negative data Due to the difficulty in characterizing the substrate site specificities of E ligases by conventional sequence logo analysis a recursively statistical method has been applied to obtain significant conserved motifs The profile hidden Markov model profile HMM was adopted to construct the predictive models learned from the identified substrate motifs A five-fold cross validation was then used to evaluate the predictive model achieving sensitivity specificity and accuracy of   and  respectively Additionally an independent testing set completely blind to the training data of the predictive model was used to demonstrate that the proposed method could provide a promising accuracy  and outperform other ubiquitination site prediction tool Conclusion A case study demonstrated the effectiveness of the characterized substrate motifs for identifying ubiquitination sites The proposed method presents a practical means of preliminary analysis and greatly diminishes the total number of potential targets required for further experimental confirmation This method may help unravel their mechanisms and roles in E recognition and ubiquitin-mediated protein degradation,
Epidemiology of appendicitis and appendectomy for the low-income population in Taiwan  Background Although numerous epidemiological studies on appendicitis have been conducted worldwide only a few studies have paid attention to the effect of socioeconomic status on appendicitis particularly studies focusing on the low-income population LIP Methods We analyzed the epidemiological features of appendicitis in Taiwan using data from the National Health Insurance Research Database from  to  All cases diagnosed as appendicitis were enrolled Results Between  and   patients from the LIP and  patients from the normal population NP were diagnosed with appendicitis Our finding revealed that the ratios of comorbidities complicated appendicitis and readmissions in LIP patients were slightly higher than those of NP patients LIP patients were more likely to live in suburban and rural areas and hence a higher proportion of them were hospitalized in a district or regional hospital compared with NP patients The crucially finding was that the overall incidence ratios of appendicitis acute appendicitis and perforated appendicitis in the LIP were substantially higher than those in the NP   and  respectively The mean LOS in LIP patients was longer than that of NP patients The overall case-fatality ratio of appendectomy in the LIP was higher when compared with the NP  versus  p lt  We also observed that appendicitis was occurred frequently in male patients with a higher incidence for those aged  years in both the LIP and NP The incidences of incidental appendectomy showed a decreasing trend in both the LIP and NP Finally a valuable discovery was that the total hospital cost was comparable between the laparoscopic appendectomy LA and open appendectomy OA    USD versus    USD p lt  in LIP patients because they saved more hospitalization costs than NP patients when the previous one chose the LA Conclusion This study confirmed that a lower socioeconomic status has significantly negative impact on the occurrence and treatment of appendicitis and appendectomy In terms of hospital costs and LOS LIP patients benefit more from the LA approach than they do from the OA approach in the treatment of appendicitis,
The Person-Event Data Environment leveraging big data for studies of psychological strengths in soldiers The Department of Defense DoD strives to efficiently manage the large volumes of administrative data collected and repurpose this information for research and analyses with policy implications This need is especially present in the United States Army which maintains numerous electronic databases with information on more than one million Active-Duty Reserve and National Guard soldiers their family members and Army civilian employees The accumulation of vast amounts of digitized health military service and demographic data thus approaches and may even exceed traditional benchmarks for Big Data Given the challenges of disseminating sensitive personal and health information the Person-Event Data Environment PDE was created to unify disparate Army and DoD databases in a secure cloud-based enclave This electronic repository serves the ultimate goal of achieving cost efficiencies in psychological and healthcare studies and provides a platform for collaboration among diverse scientists This paper provides an overview of the uses of the PDE to perform command surveillance and policy analysis for Army leadership The paper highlights the confluence of both economic and behavioral science perspectives elucidating empirically-based studies examining relations between psychological assets health and healthcare utilization Specific examples explore the role of psychological assets in major cost drivers such as medical expenditures both during deployment and stateside drug use attrition from basic training and low reenlistment rates Through creation of the PDE the Army and scientific community can now capitalize on the vast amounts of personnel financial medical training and education deployment and security systems that influence Army-wide policies and procedures,
Is dementia research ready for big data approaches The big data paradigm has gained a lot of attention recently in particular in those areas of biomedicine where we face clear unmet medical needs Coined as a new paradigm for complex problem solving big data approaches seem to open promising perspectives in particular for a better understanding of complex diseases such as Alzheimers disease and other dementias In this commentary we will provide a brief overview on big data principles and the potential they may bring to dementia research and - most importantly - we will do a reality check in order to provide an answer to the question of whether dementia research is ready for big data approaches,
Redundancy Control in Pathway Databases ReCiPa An Application for Improving Gene-Set Enrichment Analysis in Omics Studies and Big Data Biology Abstract Unparalleled technological advances have fueled an explosive growth in the scope and scale of biological data and have propelled life sciences into the realm of Big Data that cannot be managed or analyzed by conventional approaches Big Data in the life sciences are driven primarily via a diverse collection of omics-based technologies including genomics proteomics metabolomics transcriptomics metagenomics and lipidomics Gene-set enrichment analysis is a powerful approach for interrogating large omics datasets leading to the identification of biological mechanisms associated with observed outcomes While several factors influence the results from such analysis the impact from the contents of pathway databases is often under-appreciated Pathway databases often contain variously named pathways that overlap with one another to varying degrees Ignoring such redundancies during pathway analysis can lead to the designation of several pathways as being significant due to high content-similarity rather than truly independent biological mechanisms Statistically such dependencies also result in correlated p values and overdispersion leading to biased results We investigated the level of redundancies in multiple pathway databases and observed large discrepancies in the nature and extent of pathway overlap This prompted us to develop the application ReCiPa Redundancy Control in Pathway Databases to control redundancies in pathway databases based on user-defined thresholds Analysis of genomic and genetic datasets using ReCiPa-generated overlap-controlled versions of KEGG and Reactome pathways led to a reduction in redundancy among the top-scoring gene-sets and allowed for the inclusion of additional gene-sets representing possibly novel biological mechanisms Using obesity as an example bioinformatic analysis further demonstrated that gene-sets identified from overlap-controlled pathway databases show stronger evidence of prior association to obesity compared to pathways identified from the original databases,
Synthetic DNA With world wide data predicted to exceed  trillion gigabytes by  big data storage is a very real and escalating problem Herein we discuss the utility of synthetic DNA as a robust and eco-friendly archival data storage solution of the future,
SOCR data dashboard an integrated big data archive mashing medicare labor census and econometric information Introduction Intuitive formulation of informative and computationally-efficient queries on big and complex datasets present a number of challenges As data collection is increasingly streamlined and ubiquitous data exploration discovery and analytics get considerably harder Exploratory querying of heterogeneous and multi-source information is both difficult and necessary to advance our knowledge about the world around us Research design We developed a mechanism to integrate dispersed multi-source data and service the mashed information via human and machine interfaces in a secure scalable manner This process facilitates the exploration of subtle associations between variables population strata or clusters of data elements which may be opaque to standard independent inspection of the individual sources This a new platform includes a device agnostic tool Dashboard webapp httpsocrumicheduHTMLDashboard for graphical querying navigating and exploring the multivariate associations in complex heterogeneous datasets Results The paper illustrates this core functionality and serviceoriented infrastructure using healthcare data eg US data from the  Census Demographic and Economic surveys Bureau of Labor Statistics and Center for Medicare Services as well as Parkinsons Disease neuroimaging data Both the back-end data archive and the front-end dashboard interfaces are continuously expanded to include additional data elements and new ways to customize the human and machine interactions Conclusions A client-side data import utility allows for easy and intuitive integration of user-supplied datasets This completely open-science framework may be used for exploratory analytics confirmatory analyses meta-analyses and education and training purposes in a wide variety of fields,
Cloud Based Metalearning System for Predictive Modeling of Biomedical Data Rapid growth and storage of biomedical data enabled many opportunities for predictive modeling and improvement of healthcare processes On the other side analysis of such large amounts of data is a difficult and computationally intensive task for most existing data mining algorithms This problem is addressed by proposing a cloud based system that integrates metalearning framework for ranking and selection of best predictive algorithms for data at hand and open source big data technologies for analysis of biomedical data,
The National Institutes of Health's Big Data to Knowledge BDK initiative capitalizing on biomedical big data Biomedical research has and will continue to generate large amounts of data termed big data in many formats and at all levels Consequently there is an increasing need to better understand and mine the data to further knowledge and foster new discovery The National Institutes of Health NIH has initiated a Big Data to Knowledge BDK initiative to maximize the use of biomedical big data BDK seeks to better define how to extract value from the data both for the individual investigator and the overall research community create the analytic tools needed to enhance utility of the data provide the next generation of trained personnel and develop data science concepts and tools that can be made available to all stakeholders,
State of the art review the data revolution in critical care This article is one of ten reviews selected from the Annual Update in Intensive Care and Emergency Medicine  and co-published as a series in Critical Care Other articles in the series can be found online at httpccforumcomseriesannualupdate Further information about the Annual Update in Intensive Care and Emergency Medicine is available from httpwwwspringercomseries,
High-Performance Integrated Virtual Environment HIVE Tools and Applications for Big Data Analysis The High-performance Integrated Virtual Environment HIVE is a high-throughput cloud-based infrastructure developed for the storage and analysis of genomic and associated biological data HIVE consists of a web-accessible interface for authorized users to deposit retrieve share annotate compute and visualize Next-generation Sequencing NGS data in a scalable and highly efficient fashion The platform contains a distributed storage library and a distributed computational powerhouse linked seamlessly Resources available through the interface include algorithms tools and applications developed exclusively for the HIVE platform as well as commonly used external tools adapted to operate within the parallel architecture of the system HIVE is composed of a flexible infrastructure which allows for simple implementation of new algorithms and tools Currently available HIVE tools include sequence alignment and nucleotide variation profiling tools metagenomic analyzers phylogenetic tree-building tools using NGS data clone discovery algorithms and recombination analysis algorithms In addition to tools HIVE also provides knowledgebases that can be used in conjunction with the tools for NGS sequence and metadata analysis,
pvsR An Open Source Interface to Big Data on the American Political Sphere Digital data from the political sphere is abundant omnipresent and more and more directly accessible through the Internet Project Vote Smart PVS is a prominent example of this big public data and covers various aspects of US politics in astonishing detail Despite the vast potential of PVS data for political science economics and sociology it is hardly used in empirical research The systematic compilation of semi-structured data can be complicated and time consuming as the data format is not designed for conventional scientific research This paper presents a new tool that makes the data easily accessible to a broad scientific community We provide the software called pvsR as an add-on to the R programming environment for statistical computing This open source interface OSI serves as a direct link between a statistical analysis and the large PVS database The free and open code is expected to substantially reduce the cost of research with PVS new big public data in a vast variety of possible applications We discuss its advantages vis--vis traditional methods of data generation as well as already existing interfaces The validity of the library is documented based on an illustration involving female representation in local politics In addition pvsR facilitates the replication of research with PVS data at low costs including the pre-processing of data Similar OSIs are recommended for other big public databases,
Lost in Translation LiT Translational medicine is a roller coaster with occasional brilliant successes and a large majority of failures Lost in Translation  LiT beginning in the s was a golden era built upon earlier advances in experimental physiology biochemistry and pharmacology with a dash of serendipity that led to the discovery of many new drugs for serious illnesses LiT saw the large-scale industrialization of drug discovery using high-throughput screens and assays based on affinity for the target molecule The links between drug development and university sciences and medicine weakened but there were still some brilliant successes In LiT the coverage of translational medicine expanded from molecular biology to drug budgets with much greater emphasis on safety and official regulation Compared with RampD expenditure the number of breakthrough discoveries in LiT was disappointing but monoclonal antibodies for immunity and inflammation brought in a new golden era and kinase inhibitors such as imatinib were breakthroughs in cancer The pharmaceutical industry is trying to revive the LiT approach by using phenotypic assays and closer links with academia LiT faces a data explosion generated by the genome project GWAS ENCODE and the omics that is in danger of leaving LiT in a computerized cloud Industrial laboratories are filled with masses of automated machinery while the scientists sit in a separate room viewing the results on their computers Big Data will need Big Thinking in LiT but with so many unmet medical needs and so many new opportunities being revealed there are high hopes that the roller coaster will ride high again,
SeqHBase a big data toolset for family based sequencing data analysis Background Whole-genome sequencing WGS and whole-exome sequencing WES technologies are increasingly used to identify disease-contributing mutations in human genomic studies It can be a significant challenge to process such data especially when a large family or cohort is sequenced Our objective was to develop a big data toolset to efficiently manipulate genome-wide variants functional annotations and coverage together with conducting family based sequencing data analysis Methods Hadoop is a framework for reliable scalable distributed processing of large data sets using MapReduce programming models Based on Hadoop and HBase we developed SeqHBase a big data-based toolset for analysing family based sequencing data to detect de novo inherited homozygous or compound heterozygous mutations that may contribute to disease manifestations SeqHBase takes as input BAM files for coverage at every site variant call format VCF files for variant calls and functional annotations for variant prioritisation Results We applied SeqHBase to a -member nuclear family and a -member -generation family with WGS data as well as a -member nuclear family with WES data Analysis times were almost linearly scalable with number of data nodes With  data nodes SeqHBase took about secs to analyse WES familial data and approximately min to analyse WGS familial data Conclusions These results demonstrate SeqHBase's high efficiency and scalability which is necessary as WGS and WES are rapidly becoming standard methods to study the genetics of familial disorders,
Beyond Open Big Data Addressing Unreliable Research The National Institute of Health invests US  billion annually in medical research However the subsequent impact of this research output on society and the economy is amplified dramatically as a result of the actual medical treatments biomedical innovations and various commercial enterprises that emanate from and depend on these findings It is therefore a great concern to discover that much of published research is unreliable We propose extending the open data concept to the culture of the scientific research community By dialing down unproductive features of secrecy and competition while ramping up cooperation and transparency we make a case that what is published would then be less susceptible to the sometimes corrupting and confounding pressures to be first or journalistically attractive which can compromise the more fundamental need to be robustly correct,
Automated Big Data Analysis in Bottom-up and Targeted Proteomics Similar to other data intensive sciences analyzing mass spectrometry-based proteomics data involves multiple steps and diverse software using different algorithms and data formats and sizes Besides that the distributed and evolving nature of the data in online repositories another challenge is that a scientists have to deal with many steps of analysis pipelines A documented data processing is also becoming an essential part for the overall reproducibility of the results Thanks to different e-Science initiatives scientific workflow engines have become a means for automated sharable and reproducible data processing While these are designed as general tools they can be employed to solve different challenges that we are facing in handling our Big Data Here we present three use cases improving the performance of different spectral search engines by decomposing input data and recomposing the resulting files building spectral libraries from more than  million spectra and integrating information from multiple resources to select most appropriate peptides for targeted proteomics analyses The three use cases demonstrate different challenges in exploiting proteomics data analysis In the first we integrate local and cloud processing resources in order to obtain better performance resulting in more than -fold speed improvement By considering search engines as legacy software our solution is applicable to multiple search algorithms The second use case is an example of automated processing of many data files of different sizes and locations starting with raw data and ending with the final ready-to-use library This demonstrates the robustness and fault tolerance when dealing with huge amount data stored in multiple files The third use case demonstrates retrieval and integration of information and data from multiple online repositories In addition to the diversity of data formats and Web interfaces this use case also illustrates how to deal with incomplete data,
Statistical Analysis of Big Data on Pharmacogenomics This paper discusses statistical methods for estimating complex correlation structure from large pharmacogenomic datasets We selectively review several prominent statistical methods for estimating large covariance matrix for understanding correlation structure inverse covariance matrix for network modeling large-scale simultaneous tests for selecting significantly differently expressed genes and proteins and genetic markers for complex diseases and high dimensional variable selection for identifying important molecules for understanding molecule mechanisms in pharmacogenomics Their applications to gene network estimation and biomarker selection are used to illustrate the methodological power Several new challenges of Big data analysis including complex data distribution missing data measurement error spurious correlation endogeneity and the need for robust statistical methods are also discussed,
Big Data Opportunities for Global Infectious Disease Surveillance Simon Hay and colleagues discuss the potential and challenges of producing continually updated infectious disease risk maps using diverse and large volume data sources such as social media,
Big Data Versus Big Brother On the Appropriate Use of Large-scale Data Collections in Pediatrics Discussions of big data in medicine often revolve around gene sequencing and biosamples It is perhaps less recognized that administrative data in the form of vital records hospital discharge abstracts insurance claims and other routinely collected data also offer the potential for using information from hundreds of thousands if not millions of people to answer important questions However the increasing ease with which such data may be used and reused has increased concerns about privacy and informed consent Addressing these concerns without creating insurmountable barriers to the use of such data for research is essential if we are to avoid a missed opportunity in pediatrics research,
VCGDB a dynamic genome database of the Chinese population Background The data released by the  Genomes Project contain an increasing number of genome sequences from different nations and populations with a large number of genetic variations As a result the focus of human genome studies is changing from single and static to complex and dynamic The currently available human reference genome GRCh is based on sequencing data from  anonymous Caucasian volunteers which might limit the scope of genomics transcriptomics epigenetics and genome wide association studies Description We used the massive amount of sequencing data published by the  Genomes Project Consortium to construct the Virtual Chinese Genome Database VCGDB a dynamic genome database of the Chinese population based on the whole genome sequencing data of  individuals VCGDB provides dynamic genomic information which contains  million single nucleotide variations SNVs  million insertionsdeletions indels and  million rare variations together with genomic annotation information VCGDB also provides a highly interactive user-friendly virtual Chinese genome browser VCGBrowser with functions like seamless zooming and real-time searching In addition we have established three population-specific consensus Chinese reference genomes that are compatible with mainstream alignment software Conclusions VCGDB offers a feasible strategy for processing big data to keep pace with the biological data explosion by providing a robust resource for genomics studies in particular studies aimed at finding regions of the genome associated with diseases,
Big Data Analysis Using Modern Statistical and Machine Learning Methods in Medicine In this article we introduce modern statistical machine learning and bioinformatics approaches that have been used in learning statistical relationships from big data in medicine and behavioral science that typically include clinical genomic and proteomic and environmental variables Every year data collected from biomedical and behavioral science is getting larger and more complicated Thus in medicine we also need to be aware of this trend and understand the statistical tools that are available to analyze these datasets Many statistical analyses that are aimed to analyze such big datasets have been introduced recently However given many different types of clinical genomic and environmental data it is rather uncommon to see statistical methods that combine knowledge resulting from those different data types To this extent we will introduce big data in terms of clinical data single nucleotide polymorphism and gene expression studies and their interactions with environment In this article we will introduce the concept of well-known regression analyses such as linear and logistic regressions that has been widely used in clinical data analyses and modern statistical models such as Bayesian networks that has been introduced to analyze more complicated data Also we will discuss how to represent the interaction among clinical genomic and environmental data in using modern statistical models We conclude this article with a promising modern statistical method called Bayesian networks that is suitable in analyzing big data sets that consists with different type of large data from clinical genomic and environmental data Such statistical model form big data will provide us with more comprehensive understanding of human physiology and disease,
Accessing primary care Big Data the development of a software algorithm to explore the rich content of consultation records Objective To develop a natural language processing software inference algorithm to classify the content of primary care consultations using electronic health record Big Data and subsequently test the algorithm's ability to estimate the prevalence and burden of childhood respiratory illness in primary care Design Algorithm development and validation study To classify consultations the algorithm is designed to interrogate clinical narrative entered as free text diagnostic Read codes created and medications prescribed on the day of the consultation Setting Thirty-six consenting primary care practices from a mixed urban and semirural region of New Zealand Three independent sets of  child consultation records were randomly extracted from a data set of all general practitioner consultations in participating practices between  January  December  for children under years of age n Each consultation record within these sets was independently classified by two expert clinicians as respiratory or non-respiratory and subclassified according to respiratory diagnostic categories to create three gold standard sets of classified records These three gold standard record sets were used to train test and validate the algorithm Outcome measures Sensitivity specificity positive predictive value and F-measure were calculated to illustrate the algorithm's ability to replicate judgements of expert clinicians within the  record gold standard validation set Results The algorithm was able to identify respiratory consultations in the  record validation set with a sensitivity of   CI  to  and a specificity of   CI  to  The positive predictive value of algorithm respiratory classification was   CI  to  The positive predictive value of the algorithm classifying consultations as being related to specific respiratory diagnostic categories ranged from   CI  to  other respiratory conditions to   CI  to  throat infections Conclusions A software inference algorithm that uses primary care Big Data can accurately classify the content of clinical consultations This algorithm will enable accurate estimation of the prevalence of childhood respiratory illness in primary care and resultant service utilisation The methodology can also be applied to other areas of clinical care,
Putting big data to good use in neuroscience Big data has transformed fields such as physics and genomics Neuroscience is set to collect its own big data sets but to exploit its full potential there need to be ways to standardize integrate and synthesize diverse types of data from different levels of analysis and across species This will require a cultural shift in sharing data across labs as well as to a central role for theorists in neuroscience research,
 Distributed Processing of Big Data from Electrophysiological Recordings for Epilepsy Clinical Research Using Hadoop Epilepsy is the most common serious neurological disorder affecting  million persons worldwide Multi-modal electrophysiological data such as electroencephalography EEG and electrocardiography EKG are central to effective patient care and clinical research in epilepsy Electrophysiological data is an example of clinical big data consisting of more than  multi-channel signals with recordings from each patient generating GB of data Current approaches to store and analyze signal data using standalone tools such as Nihon Kohden neurology software are inadequate to meet the growing volume of data and the need for supporting multi-center collaborative studies with real time and interactive access We introduce the Cloudwave platform in this paper that features a Web-based intuitive signal analysis interface integrated with a Hadoop-based data processing module implemented on clinical data stored in a private cloud Cloudwave has been developed as part of the National Institute of Neurological Disorders and Strokes NINDS funded multi-center Prevention and Risk Identification of SUDEP Mortality PRISM project The Cloudwave visualization interface provides real-time rendering of multi-modal signals with montages for EEG feature characterization over TB of patient data generated at the Case University Hospital Epilepsy Monitoring Unit Results from performance evaluation of the Cloudwave Hadoop data processing module demonstrate one order of magnitude improvement in performance over GB of patient data Cloudwave project httpprismcaseeduprismindexphpCloudwave,
Bringing Big Data to Personalized Healthcare A Patient-Centered Framework ABSTRACT Faced with unsustainable costs and enormous amounts of under-utilized data health care needs more efficient practices research and tools to harness the full benefits of personal health and healthcare-related data Imagine visiting your physicians office with a list of concerns and questions What if you could walk out the office with a personalized assessment of your health What if you could have personalized disease management and wellness plan These are the goals and vision of the work discussed in this paper The timing is right for such a research directiongiven the changes in health care reimbursement reform meaningful use of electronic health care data and patient-centered outcome mandate We present the foundations of work that takes a Big Data driven approach towards personalized healthcare and demonstrate its applicability to patient-centered outcomes meaningful use and reducing re-admission rates,
DIVE A Graph-based Visual Analytics Framework for Big Data The need for data-centric scientific tools is growing domains like biology chemistry and physics are increasingly adopting computational approaches As a result scientists must now deal with the challenges of big data To address these challenges we built a visual analytics platform named DIVE Data Intensive Visualization Engine DIVE is a data-agnostic ontologically-expressive software framework capable of streaming large datasets at interactive speeds Here we present the technical details of the DIVE platform multiple usage examples and a case study from the Dynameomics molecular dynamics project We specifically highlight our novel contributions to structured data model manipulation and high-throughput streaming of large structured datasets,
Using big data to map the network organization of the brain The past few years have shown a major rise in network analysis of big data sets in the social sciences revealing non-obvious patterns of organization and dynamic principles We speculate that the dependency dimension  individuality versus sociality  might offer important insights into the dynamics of neurons and neuronal ensembles Connectomic neural analyses informed by social network theory may be helpful in understanding underlying fundamental principles of brain organization,
Spatial Big Data Analytics of Influenza Epidemic in Vellore India The study objective is to develop a big spatial data model to predict the epidemiological impact of influenza in Vellore India Large repositories of geospatial and health data provide vital statistics on surveillance and epidemiological metrics and valuable insight into the spatiotemporal determinants of disease and health The integration of these big data sources and analytics to assess risk factors and geospatial vulnerability can assist to develop effective prevention and control strategies for influenza epidemics and optimize allocation of limited public health resources We used the spatial epidemiology data of the HIN epidemic collected at the National Informatics Center during - in Vellore We developed an ecological niche model based on geographically weighted regression for predicting influenza epidemics in Vellore India during - Data on rainfall temperature wind speed humidity and population are included in the geographically weighted regression analysis We inferred positive correlations for HN influenza prevalence with rainfall and wind speed and negative correlations for HN influenza prevalence with temperature and humidity We evaluated the results of the geographically weighted regression model in predicting the spatial distribution of the influenza epidemic during -